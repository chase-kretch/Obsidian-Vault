
### Lecture 04 Virtual Machines 2

- Virtualisation Implementation
	- Generally difficult to make a exact duplicate of a machine
		- Especially if only dual mode on CPU
		- CPU features and support for VMMs (Virtual Machine Manager), it is getting easier
	- Most VMMs implement virtual CPU to represent states of each VM
		- When guest context switched onto CPU by manager, information from VCPU is loaded and storage
- Trap and Emulate
	- Dual mode CPU: virtual machine guest executes in usermode
	- Just as the physical machine has two modes, so much the VM
		- VM needs two modes, V usermode and V Kernel mode, both run in real user mode
	- Actions in guest that usually cause switch to kernel mode must cause switch to VKM
	- Attempting priviledge instruction in user mode causes and error (trap)
	- VMM gains control, checks error, and executes operation as attempted by guest (emulate)
	- Return control to guest in user mode
	- User mode code in guest runs at same speed as if not a guest
		- Kernel mode code instead runs slower due to trap and emulate
		- Especially when multiple guests are running, (each need trap and emulate)
	 - Now that there is support for VMMs, CPUs tend to do this far better.
- Virtualization Problems
	- Some cpus dont have clean separation between privileged and nonprivileged instructions
		- Earlier x86 CPUs
		- x86 popf
			- Load CPU flags register from contents of the stack
			- If CPU in privileged mode, all flags replaced
			- If user mode, only some are replaced
			- No trap generated
	- Special Instructions caused:
		- Until 1998, trap-emulate was considered impossible
		- **Binary Translation** solves this
			- If guest VCPU is in usermode, guest can run instructions natively.
			- If kernel mode,
				- VMM examines every instruction guest is about to execute by reading ahead
				- Non special instructions run natively
				- Special instructions translated at runtime into new instructions that do equivalent task
		- Not slow because
			- Translation is simple
			- Only translates code that is ran
			- Much is the same as original
			- Cached and reused
			- Other tricks
	- **Memory Management**
		- Another challenge.
		- How can a VMM keep page table state for both guests believing they control the page and VMM that controls the tables
		- Commonly used **nested page tables (NPTs)**
			- For trap and emulated and binary translation.
			- Each guest maintains page tables to translate virtual to physical address
			- VMM maintains per guest NPT to represt guest page table
			- Just as VPU stores guest CPU state
			- When guest CPU, VMM makes the guests NPT the active page table
			- Causes TLB misses, slow performance
	- **Hardware**
		- All virtualisation needs some HW support
		- Intel added VT-x AMD AMD-V (2005 2006)
			- CPUs with this dont need binary translation
			- Defines more CPU modes, guest host
		- HW support for NPT, DMA and interrupts.
- **More VM styles**
	- Application Containment (OS Level Virtualisation)
		- Good for servers (use same OS)
		- Containers look like servers, they can be rebooted separately, have their own ip, root, programs, but use **same kernel**.
	- Paravirtualisation
		- Xen - requires modifying OS source code to use xen layer, guest knows it is running off a host.
	- Application Virtualisation
		- Application runs on a layer which provides resources it needs even though it may be running on a different OS. Wine for running programs from old windows on a newer one. (Wine is not an emulator)
	- Programming-Environment Virtualisation
		- Java Vm, CLR/MONO
		- Implement different architecture on top of hardware /OS
	- Windows Subsystem for Linux 

## Lecture 05 Systems Programming Language

- C is the language of choice for majority of OS.
	- High level assembler, can do low level things such as accessing and modifying memory.
	- Also is portable (can be used on different architectures)
	- ![[Pasted image 20240729012552.png]]
- Accessing Registers
	- You can use the register storage class specifier to say that a variable should be kept in the register e.g
		- register long number = 1234
	- However this doesnt guarantee that the value is stored in the register, it depends on how many available registers there are
	- Also, compilers are good at optimising register usage and it isnt a good idea for a programmer to manually do this
	- memory mapped registers can be accessed directly using pointer manipulation
- Volatile
	- Volatile is a type qualifier
		- volatile unsigned char *reg
		- This means the variable may change in a non local way, or that the compiler has no way of knowing whether the value has changed or not between references
		- Compiler is not allowed to optimise accesses. Ever read must go back to the main store to retrieve the current value
			- memory mapped device registers
			- values modified in interrupt routines
			- values modified in another thread
- Memory Management
	- All local variables disappear when functions are returned
		- Space is allocated on the stack for the variables in each function invocation
		- There is a limit to stack size soft limites asnd hard limits. 
	- Areas of static memory
		- Global Variables
		- Static variables
		- If in a function it maintains its value even when the function is returned. Where is the variable actually stored?
	- The advantage of static memory is that it is allocated at compile time and hence has no allocation overhead at ruin time.
	- The disadvantage is that it cannot easily be released.
- Dynamic Memory
	- C requires explicit control of dynamic memory
		- This is suitable for OS programming as there is no garbage collection available
		- garbage collection adds a layer of complexity and unpredictability to the programming environment
		- this is important in small systems such as embedded systems (or phones)
		- especially important in real-time syustems
		- malloc or callic
		- free(thread)

## Lecture 6

Processes and Threads

- Process
	- The thing which represents our work to the systemm
	- Sometimes referred to as a heavyweight process
		- I instance of a program in execution
	- May be more than one version of the same program running at the same time (hopefully sharing the code), each instance has resources limitations, security information, rights, capabilities etc.
	- So it includes code, data, connections (to files networks and other processes) access to devices
	- It needs the processor to run, but it doesnt run all the time. So it needs information about what it is up to stored somewhere
- Two parts to a process
	- 1, Resources, the things that the process owns (may be shared) also information about the process
	- 2, What the process is doing - streams of execution
	- Traditin processes had resources and a single current location e.g traditional UNIX. The resource part is called a task or a job. The location part is commonly called a thread
	- Most operating systems now provide support to keep these parts separate e.g Linux, Solaris, Windows and MacOS
- Threads
	- Sometimes referred to as lightweight processes
	- A sequence of instuructions being executed when there is no external intervention.
	- Multiple threads of a process, share the prcess resources but have their own thread ID, PC, registers and stack
	- Easier to create than a process. They provide a nice encapsuilation of a problem within a process rather than multiple processes
	- Easier to switch between threads than between processes.
	- Typical Uses
		- Splitting work across processors (shared memory multiprocessor, multiple cores)
		- added responsiveness (handle user input while still finishing another function)
		- controlling and monitoring other threads
		- server applications
		- can help with program abstraction
	- Thread implementation
		- User-level (green threads)
			- The OS only sees one thread per process
			- The process constructs other threads by user-level library calls or by hand
			- user-level control over starting and stopping threads
			- usually a request is made to the OS to interrupt the process regularly (an alarm clock) so that the process can schedule another thread
			- The state of threads in the library code does not correspond to the state of the process
		- System-level
			- The OS knows about multiple threads per process 
			- Threads are constructed and controlled by system calls
			- The system knows the state of each thread
		- User-level advantages
			- Works even if the OS doesnt support threads
				- Some implementations of Java had user-level threads because the underlying OS didnt
			- Easier to create- no system call
				- Just normal library procedure call
				- No switch into kernel mode (this saves time)
			- Control can be application specific
				- Sometimes the OS doesn't give the type of control an application needs
				- e.g Precise priority levels, changing scheduling decisions according to state changes
			- Easier to switch between - saves processor mode changes
				- Can be as simple as saving and loading registers (including SP, PSW and PC)
			- The reason this isn't always better  than system-level threads is because the OS only sees one thread per process so e.g if the thread is waiting for I/O the whole system will wait for I/O
		- System-Level thread advantages
			- Each thread can be treated separately
				- Rather than using the timeslice of one process over many threads
				- Should a process with 100 threads get 100 times the CPU time of a process with 1 thread.
					- No! All threads have their individual CPU time and priority
			- A thread blocking in the kernel doesn't stop all other threads in the same process
				- With user-level threads if one thread blocks for I/O the OS sees the process as blocked for I/O even if there are other threads running
			- On a multiprocess (including multi-core) different threads can be scheduled on different processores
				- This can only be done if the OS knows about the threads
				- Even then it sometimes doesnt work -standard Python has system level threads but the Global Interpreter Lock (GIL) means that only one runs at a time even on a multicore machine
		- Which of these can't be done with User-level threads?
			- Splitting work across processes (shared memory multiprocessir, multiple cores)
				- No! Because the operating system sees the process as one
			- Added responsiveness (handle user input while finishing another function)
				- No (atleast not efficient), the OS will view the user-input threads of a process a single thread and once one is waiting for I/O the process will be considered blocked/waiting
			- Controlling and monitoring other threads
				- Is possible at user level as we dont need multiple processes running
			- Server Applications
				- It is possible with user level threads but to use multithreading and multicore CPUs we need server threads
			- Can help program abstraction
				- Yes, both system and user level are useful for concurrent programs
	- Jacketing
		- One major problem is user-level threads is the blocking of all threads within a process when one blocks
		- A possible solution is known as jacketing
			- A blocking system call has a user-level jacket
			- The jacket checks to see if the resource is available e.g device is free
			- If not another thread is started
			- When calling thread is scheduled again (by the thread library) it once again checks the state of the device
		- So there has to be some way of determining is resources are available to accept requests immediately
	- The best of both worlds
		- Solaris (versions < 9) had both user-level and ssytem-level threads.
		- LWP (light weight process) what we call system level threads)
		- Kernel Threads - active within the kernel
			- Each LWP is associated with one kernel thread
		- One or more user threads could be multiplexed on each LWP
		- A process could have several user and several LWPs
		- The number of LWPs per process as adjusted automatically to keep threads running
			- If all LWPs in process block, but there are user level threads which could run kernel creates new LWP
		- ![[Pasted image 20240806020610.png]]
	- Original Linux Threads (before 2.6)
		- Thread creation is done through clone() system call.
		- Clone() makes a new process
			- Shares memory address space, open files, signal handlers)
		- From one point of view original Linux threads were processes but they shared all resources and hence the advantages of threads
		- original LINUX threads and POSIX
			- Cant be set to schedule according to priority within a process
				- Each thread is scheduled independently across all threads/processes in the system
			- Cant send a signal to the whole process
			- Ordinary system calls e.g read were nto cancellation points
			- Starting a new program in one thread doesn't kill the other threads in the same process
			- When an original Linux thread blocks doing IO do all other threads in the same process stop?
## Lecture 7
 Memory Layout of C Program
- PThread Programming
	- POSIX threads or ptrheads are used in UNIX type operating systems
	- Use "man pthread_create" to get the manual page
	- ![[Pasted image 20240806171331.png]]
	- The #include needs to be added to the top of your source file
	- going through the parameters
		- thread is a pointer to a pthread_t type - this is where information about the thread is stored, we need this to join the thread when we wait for it to finish
		- attr is a pointer to pthread attributes, to use the default we put NULL
		- start_routine is the name of the function which the thread runs
		- arg is a pointer to arguments/parameters used by the function
	- Making sure a PThread has finished
		- ![[Pasted image 20240806172455.png]]
	- A thread function
		- To keep our compiler happy our thread function must be of the type:
			  void \*(\*start_routine) (void \*)
		- The start_routine is a pointer to a function. In C this is exactly the same as a function name
		- The function returns a pointer to a void (i.e, it can return a pointer to anything, we can cast it)
		- The function accepts one parameter which is also a pointer to a void (i,e we can make it point to anything)
			- void \*the_function(void \*params) {...}
		- We run this in a thread by calling:
			- pthread_create(&thread_info, NULL, the_function, (void \*) &args)
			- Where args is a pointer to the values we want to send to the function, and thread_info is a pthread_t.
				- pthread_t thread_info;
		- ![[Pasted image 20240806173300.png]]
- Memory Layout of a C Program
	- ![[Pasted image 20240806173422.png]]
	- Segmentation Fault
		- Each region has its own set of permissions. For example. the code segment is usually marked as real-only to prevent accidental modification of the program code.
		- Situations we may get a segmentation fault:
			- Accessing unitialized memory
			- Dereferencing a null pointer
			- Stack overflow
			- Accessing memory outside of its allocated space
	- Address translation
		- Virtual address spaces = pages
		- Physical address space = frames
		- How do we map virtual to physical
		- Page tables map virtual to physical
	- Benefits of Memory Mapping
		- Efficient use of physical memory
			- By allowing multiple processes to share the same physical memory, the operating system can use memory more efficiently
		- Protection
			- Each process has its own virtual memory space, which provides protection against other processes accessing its memory
		- large address space
			- Virtual memory allows each process to have a larger virtual address space than is available in physical memory
		- 
			  
## Lecture 8
- Process Control Block (PCB)
	- Information associated with each process (also called task control block)
	- Where the OS can find all the information it needs to know about a process
		- Process identified and manged via a process identifier (pid)
		- Process state, running waiting, etc
		- Program counter
		- CPU registers
			- contents of all process-centric registers
		- CPU scheduling information
			- priorities, scheduling queue pointers
		- Memory-management information
			- memory allocated to the process
		- Accounting information
			- CPU used, clock time elapse since start, time limits
		- I/O status
			- I/O devices allocated to process, list of open files
	- Doesnt have to be kept together
	- ![[Screenshot 2024-08-01 at 2.14.05 PM.png]]
- UNIX PCBs
	- The PCB is the box labelled process structure but the user structure maintains some of the information as well (only required when the process is resident)
	- ![[Screenshot 2024-08-01 at 2.16.05 PM.png]]
- Windows NT PCBs
	- Information is scattered in a variety of objects
	- Executive Process Block (EPROCESS) includes:
		- KPROCESS and PEB (Process Environment Block)
			- KPROCESS includes info the kernel needs to scheduele threads
				- Kernel and user times
				- Pointers to threads
				- Priority information
				- 
			
		- Points to threads
		- Priotity information
- ![[Screenshot 2024-08-01 at 2.23.03 PM.png]]
- Linked together with a doubley linked list
- Process State
	- As a process executes it changes state
		- New: the process is being created
		- Running: being executed
		- Waiting: pending for some event to occur
		- Ready: the process is waiting to be assigned to a processor
		- Terminated: finished executing
		- ![[Screenshot 2024-08-01 at 2.25.22 PM.png]]
- Process Creation
	- Different methods of creating processes
		- Create process system call
			- takes a program name or a stream with the program data
			- windows
		- copy process system call
			- A strange way of doing it but is now very widespread thanks to UNIX
		- Create a new terminal session
		- Requires
			- A spare or new PCB
			- mark it new
			- generate a unique identifier
			- get some memory (what if there isnt any) or
			- fill in page table entries
			- set up PCB fields with initial values
			- set priority and resource limits
			- change state to ready
			- can be done by inserting into a queue of runnable processes
		- What about other resources?
			- Some OS' carefully allocate resources before a process runs (this prevents deadlock later)
			- Others leave these to the process to collect as it runs
		- Address space
			- Child duplicate of parent 
				- each one has its own copy of any data
				- Child has a new program loaded into it
			- UNIX Examples
				- fork() system call creates a new process
				- exec() system call used after a fork() to replace the processes memory space with a new program
				- Parent process calls wait() waiting for the child to terminate
				- ![[Screenshot 2024-08-01 at 2.35.45 PM.png]]
	- UNIX Fork() Call
		- Parent process
			- the one who made the call
		- Child
			- new process
		- Traditionally memory was duplicated - the code was shared even from earliest days
			- Share open files as well
			- Open file information will have the count of processes using them increases by one
			- And shared memory regions
			- Fork returns 0 in the child process and the childs pid in the parent
			- ![[Screenshot 2024-08-01 at 2.40.05 PM.png]]
			- ![[Screenshot 2024-08-01 at 2.48.51 PM.png]]
			- 6 times
	- Exec() System call
		- Checks to see if the file is executable
		- saves any parameters in some system memory
		- releases currently held memory
		- loads the program
		- moves the save parameters into the stack space of the new program
		- ready to run again
	- Fork used to copy the data memory of the process
	- If the child is going to do na exec this is a waste of effort
	- particularly bad with virtual memory

## Lecture 9

- Solutions to fork() using memory
	- copy on write
		- No copy is made at first
		- The data pages of the parent process are set of read only
		- If a write occurs the reuslting exception makes a copy of the page for the other process
		- preferred way nowadays
	- vfork()
		- needs programmers to know what they are doing
		- doesnt copy entire address space
		- let child work on parent address space
		- parent process blocks until child finishes or calls exec.
- ![[Pasted image 20240818230829.png]]
- Runnable
	- On a single core, only one process/thread can run at a tme (not always true simultaneous multithreading (SMT) or hyperthreading)
		- Many may however be runnable - either running or ready
- Pre-emptive multitasking
	- A clock interrupt causes the OS to check to see if the current thread should continue
		- Each thread has a time slice
	- What advantages/disadvantages does preemptive multitasking have over cooperative multitasking
		- Advantages
			- control
			- predictability
		- disadvantages
			- Critical sections (stopping during important stuff)
			- Efficiency
- Cooperative Multitasking
	- Two main approaches
		- A proces yields its right to run (yield() in java)
		- System stops a process when it makes a system call
	-  This doesnt mean a task will work to completion without allowing another process to run.
	- A mixture
		- Older versions of UNIX, did not allow preemptive multitasking when a process made a system call (ie. entered the kernel)
- Context Switches
	- It is the chance from one process running to antoher running on the same processer is usually referred to ass a context-switch
	- What is the context (the state associated with the process)
		- registers
		- memory - including dynamic elements such as the call stack
		- files, resources
		- but also things like caches, TLB (translation lookaside buffer) values, these are normally lost
	- Context of a process is represented in the PCB (process control block)
	- The context changes as the process executes
	- But normally a context switch means the chance from one process/thread running to another, or from a process/thread running to handling an interrupt. Whenever the process state has to be stored and restored
	- ![[Pasted image 20240818232208.png]]
	- Back to running after context switch
		- State transition
			- Must store process properties so it can restart where it was
			- if changing processes the page table needs altering
			- Rest of the environment must be restored
			- if changing threads within the same process, simply restoring the registers might be enough
			- Some systems have multiple sets of registers which means that a thread change can be done with a single instruction
	- Waiting
		- Processes seldom have all the resources they need when they start
			- Memory
			- Data from files or devices (i/o)
		- waiting processes must not be allowed to unnecessarily consume resources, in particular the processor.
			- state is changed to waiting
				- more than one type of waiting state
				- short wait e.g for memory
				- long wait e.g for archied file
			- removed from the ready queue
			- probably entered on a queue for whatever it is waiting for
		- When the resource becomes available
			- state is changed to ready
			- removed from waiting queue
			- put back on the ready queue
	- Suspended
		- Another type of waiting
			- ctrl-z in unix shell
		- Operators or OS temporarily stopping a process ie it is not normally caused by the process itself
			- allows others to run to completion more rapidly
			- or to preserve the work done if there is a system problem
			- or to allow the user to restart the process in the background
		- Suspended processes are commonly swapped out of real memory
			- This is a state which affects the process not individual threads
	- Why we dont use Java suspend()
		- If dealing with threads in Java we dont use these deprecated methods:
		- suspend() freezes a thread for a while
		- resume() releases the thread
		- But we can easily get deadlock
			- suspend() keeps hold of all locks gathered by the thread
			- if the thread which was going to call resume() needs one of those locks before it can proceed we get stuck
	- Why we dont use stop
		- stop() kills a thread forcing it to release any locks it might have
			- we will see where locks come from in later lectures
		- the idea of using locks is to protect some shared data being manipulated simultaneously
		- if we use stop() the data may be left in an inconsistent state when a new thread accesses it.
	- Waiting in some UNIXes
		- A process wiating is placed on queue (originally used to scan whole process table)
		- The queue associated with the has value of a kernel address (waiting or suspended processes may be swapped out)
			- when the resource becomes available
			- all thngs waiting for that resource are woken up
			- (may need to swap the process back in)
			- first one to run gets it
			- if not available when a process runs the process goes back to waiting
			- a little like in Java
				- while(notAvailable)
					- wait();

## Lecture 10

Inter Process Communication

- Process Termination
	- Process executes last statement and then asks the operating system to delete it using the exit() system call
		- Returns status data from child to parent (via wait())
		- pid = wait(&status)
	- Parent may terminate the execution of children processes using the abort() system call
	- Some reasons for doing so:
		- child has exceeded allocated resources
		- Task assigned to child is no longer required
		- The parent is exiting, and the operating systems does not allow a child to continue if its parent terminates (cascading termination - initiated by OS)
	- All resources must be accounted for
		- may be found in the PCB or other tables e.g devices, memory, files I/O buffers
	- reduce usage count on shared resources
		- memory, libraries, file/buffers
	- if the process doesnt tidy up e.g close files, then something else must
	- accounting information is updated
	- remove any associated processes
		- was this a session leader? if so then all processes in the same session be removed
	- remove the user from the system
	- notify the relatives
- Unix Stopping
	- Usually call exit(termination status)
	- open files are closed - including devices
	- memory is freed
	- accounted updated
	- if no parent waiting (did not invoke wait()) process is a zombie
	- if parent terminated without invoking wait(), process is an orphan
	- children get init as a step-parent
	- parent is signalled (in case it is waiting or will wait)
	- after the parent retrieves the termination status the PCB is freed)
- Interprocess Communicaton
	- Processes within a system may be independent or cooperating
	- Cooperating process can affect or be affected by other processes
	- Reasons for Cooperating processes:
		- Information sharing
		- Computation Speedup
		- Modularity
	- Cooperating processes need interprocess communication (IPC)
	- Two models of IPC
		- Shared memory (require process synchronization)
		- Message Passes
	- Shared memory
		- Faster
	- Message Passing
		- Slower
		- More Controlled
		- Secure
		- Needs:
			- Some way of addressing the message
			- SOme way of transporting the message
			- Soem way of notifying the receiver that a message ahs arrived
			- Some way of accessing or depositing the message in the receiver
		- Message size is either fixed or variable
		- send(destination, message)
		- receive(source, message)
		- write(message)
		- read(message)
		- In this case we also need some way of making a connection between the processes like an open() call.
	- Design Decisions
		- Implementation issues
		- HOw are linked established
		- Can a link be associated with more than two processes
		- How many links can there be between every pair of communicating processes
		- What is the capacity of a link
		- Is the size of a message that the link can accomodate fixed or variable
		- Is a link unidirectional or bi-directional
		- Should the sender block until the message is received
		- Should the receiver block until the message is available
		- If both send and receive are blocking (synchronous communication) we have a rendezvous
	- Direct Communication
		- Processes must name each other explicitly
			- send(destination, message)
			- receive(source, message)
		- Properties of communication kubj
			- links are established automatically
			- One link between each pair of processes
			- Link may be unidirectional but is usually bi-directional
			- Receiver doesnt have to know the id of the sender (it can receive it with the message)
				- A server can receive from a group of processes
			- Disadvantages
				- Cant easily change the names of processes
				- Could lead to multiple programs needing to be changed
	- Indirect Communication
		- Messages are directed and received from maleboxes (also referred to as ports)
		- Each mailbox has a unique Id
		- processes can communicate only if they share a mailbox
		- Properties of communication link
			- Link established only if processes share a common mailbox
			- A link may  be associated with many processes
			- Each pair of proceesses may share several communication links
			- link may be unidirectional or bi-directional
		- mailbox ownership
			- System owned
			- Process Owned
				- What happens to other processes if the owner process gets deleted
	- Ordinary Pipes (UNIX)
		- Act as a conduit allowing two processes to communicate
		- Issues
			- Unidirectional
			- In the case of two-way commucation is it half or full duplex? NOT for pipes
			- Must there exist a relationship (parent-child) between the communicating processes? NO
			- Can pipes be used over a network? NO
		- Named pipes can do all
		- int my_pipe[2]; pipe(my_pipe);
		- Data gets put into the pipe and taken out the other end
		- write = mypipe[1]
		- read = mypipe[0]
		- use ordinary read() write() system calls
		- e.g write(mypipe[1], data, length)
		- Cannot be accessed from outside the process that created it
		- require parent-child relationship between communicating processes
		- Windows calls these anonymous pipes.
		- Empty and full pipes
			- Reading process blocks when pipe is empty (until data becomes available)
			- Writing process blocks when pipe is full (65536 bytes on recent Linux)
		- Broken pipes
			- A process wiating to read from a pipe with no writer gets an EOF (once all existed data has been read)
			- A process writing to a pipe with no reader gets signalled
			- Writes are guaranteed to not be interleaved if they are smaller than the PIPE_BUF constant. THis must be atleast 512 bytes and is 4096 bytes by default on Linux.

## Lecture 11

- Communicating via shared resources
	- Shared resources
		- Separate processes can alter the resource
		- Need to check the state of the resource to either receive data or know that some event has occured
		- Usually ned to explicitly coordinate access to the resource
	- Files
		- Easy to use but slow
		- File system may provide synchronisaton help, e.g only one writer at a time
	- Memory
		- Fast
		- Synchronisation is usually handled explicitly by the process
- Shared Memory
	- Different threads in the same process automatically share memory. How can we share memory between different heavyweight processes?
		- define sections of shared memory
		- attach the shared memory to the process
		- Detach, indicate who can access it etc.
	- Both processes need to know the name of the area of the shared memory
	- Must make sure the memory is attached to some unused area of the process' address space
	- Usual security checks - can this process attach to this chunk of memory
	- What about if the processes are on separate machines
- POSIX Shared memory
	- To share memory between unrelated processes
	- Process first creates shared memory segment
		- shm_fd = shm_openname, OCREATE | O_RDWR, 0666)
	- also used to open an existing segment
	- Set the size of the object
		- ftruncate(shm_fd, 4096)
	- Use mmap() to memory-map a file pointer to the shared memory object
	- Reading and writing to shared memory is done by using the pointer returned by mmap()
- Signals
	- Signals are used in UNIX systems to notify a process that a particular event has occured
	- A signal handler is used to process signals
		- Signal is generated by particular event
		- An event is such as a child has finished processing or invalid memory allocation
		- Signal is delivered to a process
		- Signal is handled b y one of two signal handlers:
			- default
			- user-defined
	- Every signal has default handler that kernel runs when handling signal
		- User-defined signal handler can override default
		- for single-threaded, signal delivered to process
		- For multi threaded, depends on type of signal generated
	- Signals are like software interrupts
	- kill(pid, signalNumber);
	- originally for sending events to the process because it had to stop
	- signalNumbers for:
		- illegal instructions
		- memory violation
		- floating point exceptions
		- children finishing
		- job control
		- broken communicaton
		- keyboard interrupt
		- loss of terminal
		- change of window size
		- user defined
	- But processes can catch and handle signals with signal handlers
	- signal(signalNumber, handler);
	- Can also ignore or do nothing if you dont ignore or set a handler, then getting a signal stops the process
	- One signal cant be handled - 9 SIGKILL/SIGSTOP
- Mach Ports
	- Underneath macOS and iOS
		- Has 2 kernels, mach kernel and BSP UNIX kernel
	- Everything is done via ports even system calls and exception handling
	- Only one receiver from each port but can have multiple senders (unidirectional too)
	- Can pass the right to receive
	- WHen a process is started, it is given send rights to a port on the bootstrap process (the service naming daemon, normally gives the bootstrap process send rights via a message)
	- Programmers dont usually work at that level (they can use the standard UNIX communication mechanisms)
	- Each task gets two ports at creating, kernel and notify
	- Messages are sent and received using the mach_msg() function
	- Ports needed for communication created via
		- mach_port_allocate()
	- Send and receive are flexible, for example four options if mailbox full:
		- Wait indefinitely
		- Wait at most n milliseconds
		- return immediately
		- Temporarily cache a message
- Local Procedure Calls (Windows)
	- Message passing centric via advanced local procedure call (ALPC) facility
		- Only works between processes on the same system
		- Uses ports (like mailboxes) to establish and maintain communication channels
		- Communication works as follows
			- The client opens a handle to the subsystems connection port object
			- The client sends a connection request
			- The server creates two private communication ports and returns the handle to one of them to the client
			- The client and server use the corresponding port handle to send messages or callback and to listen for replies

## Lecture 12

CPU Scheduling

- Basic Concepts
	- Maximum CPU itlisation obtained with multiprogramming
	- CPU I/O Burst Cycle - Process execution consists of a cycle of CPU execution and I/O wait
	- CPU Burst followed by I/O burst
	- CPU burst distribution is of main concern
- Histogram of CPU Burst Times
	- Large number of short CPU bursts
	- Small number of longer CPU bursts
	- ![[Pasted image 20240819174146.png]]
- CPU Scheduler
	- The CPU scheduler selects from among the processes in ready queue, and allocates a CPU core to one of them
		- Queue can be ordered in various ways
	- CPU scheduling decisions may take place when a process
		- Switches from running to waiting state
		- Switches from running to ready state
		- Switches from waiting to ready
		- Terminates
	- For situations 1 and 4, there is no choice on terms of scheduling - non preemptive or cooperative scheduling
	- For situations 2 and 3 however there is a choice - preemptive scheduling
	- Preemptive scheduling can result in race conditions
- Preemptive and NonPreemptive Schedling
	- Under Cooperative scheduling, once the CPu has been allocated to a process, the process keeps the CPu until it releases it either by terminating to process or switching to the waiting state
	- Preemptive scheduling can result in race conditions when data are shared among several processes
	- Consider the case of two processes that share data. While one process is updating the data, it is preempted so that the second process can run. The second process then tried to read the data which are in an inconsistent state
	- Virtually all modern operating systems including Windows, MacOS, Linux and UNIX use preemptive scheduling algorithms
- Dispatcher
	- Dispatcher module gives control of the CPU to the process selected by the CPU scheduler. This involves
		- Switching context
		- Switching to user mode
		- Jumping to the proper location in the user program to resume that program
	- Dispatch latency
		- Time it takes for the dispatcher to stop one process and start another running
- Scheduling Criteria
	- Maximise CPU utilisation
		- Keep the CPU as busy as possible
	- Maximise Throughput
		- Number of processes that complete their execution per time unit
	- Minimise Turnaround time
		- Amount of time to execute a particular process
	- Minimise Waiting time
		- Amount of time a process has been waiting in the ready queue
	- Minimise Response time
		- Amount of time it takes from when a request was submitted until the first response is produced
- First Come First Served (FCFS)
	- Reduces waiting time
	- No time wasted to determine which process should run next
	- Little overhead as context switch only when required
	- Non-preemptive
	- ![[Pasted image 20240819175416.png]]
	- Convoy Effect - short process behind long process
		- Consider one CPU-bound and many I/O-bound processes
		- Results in lower CPU and device utilisation
- Round Robin (RR)
	- pre-emptive version of FCFS
	- Need to determine the size of the time slice or time quantum
	- What is wrong with treating every process equally?
		- no concept of priorities
		- doesnt deal with different types of process, computer bound vs IO bound
	- One way to tune this is to change the length of the time slice
		- Less time using for context switches in long time slice
		- short time slice has a lot of context switches
- Shortest Job first (SJF)
	- SJF is optimal, gives minimum average waiting time for a given set of processes
	- Unfortunately, we dont know which process has the shortest CPU burst
	- Use the previous CPU bursts to estimate the next
	- Preemptive  SJF
		- Whenever a process arrives in the ready queue, with a shorter burst time then the remaining burst time of the running process then the process is preempted
		- Adds an arrival time value
		- Waiting time only starts from when the process arrives
		- ![[Pasted image 20240819180624.png]]
- Priority Scheduling
	- ![[Pasted image 20240819180802.png]]
- Handling Prioities
	- Explicit Priorities
		- Unchanging
		- Set before a process runs
		- When a new process arrives, it is placed in the position in the ready queue corresponding to its priority
		- It is possible to get indefinite block or starvation (low priority processes may never execute)
	- Variable Priorities
		- Priorities can vary over life of the process
		- The system makes changes according to the process' behaviour: CPU usage, IO usage, memory requirements etc
		- If a process is not running because it has a low priority
			- Solution - Aging (as time progresses increase the priority of the process)
			- or a process of a worse priority might be schedule after five processes of a better priority
			- This prevents starvation but better priority processes will still run more often

## Lecture 13

- Multi-Level queue scheduling
	- The ready queue consists of multiple queues
	- Multilevel queue scheduler defined by the following parameters
		- Number of queues
		- Scheduling algorithms for each queue
		- Method used to determine which queue a process will enter when that process needs service
		- Scheduling among the queues
		- A process stays on its original queue or can be moved from queue to queue
- Multiple-Processor Scheduling
	- Symmetric multiprocesses (SMP) is where each processor is self scheduling
	- Two kinds
		- All threads may be in a common ready queue
		- Each processor may have its own private queue of threads
	- ![[Pasted image 20240819222849.png]]
- Real time CPU Scheduling
	- Can present obvious challenges
	- Soft real-time systems - critical real-time tasks have the highest priority but no guarantee as to when tasks will be scheduled
	- Hard real-time systems - tasks must be serviced by its deadline
	- When processes are submitted, they indicate their CPU requirements
	- The scheduler may reject the process if the requirement cannot be met
	- But very important processes can force other processes to relinquish their allocations
	- Event latency - the amount of time that elapses from when an event occurs to when it is services
	- Two types of latencies affect performance
		- Interrupt latency - time from arrival of interrupt to start of routine that services interrupt
		- Dispatch latency - time for schedule to take current process off CPU and switch to another (context switch)
			- Conflict phase of dispatch latency
				- Preemption of any process running in kernel mode
					- Will only occur after process exits critical section
				- Release by low-priority process of resources needed by high-priority resources
	- For real time schedling, the job is the scheduler must support preemptive, priority -based scheduling
		- But only guarantees soft real-time functionality
		- No guarantee for itll meet deadline
	- For hard real-time must also provide ability to meet deadlines
	- (c,p,d) tuple
		- computation time (worst case)
		- period time
		- deadline
		- c <= d <= p
- Periodic Process
	- require CPu at constant intervals
	- used for polling, monitoring and sampling
	- predetermined amount of work every period
	- Period and deadline are determined by the system requirements (often the same)
	- Computation time is found through analysis, measurement or simulation
	- When the computation is complete the process is blocked util the next period starts
	- Sometimes it doesnt matter if the deadline extends period time, or the period can change depending on system load.
- Sporadic Process
	- event driven - some external signal or change
	- used for fault detection, change of operating modes
	- (c,p,d) still applies
	- c and d have the obvious meaning
	- p is the minimum time between events
	- aperiodic processes
	- p = 0
	- events can happen at any time, even simultaneously
	- timing can no longer be deterministic but there are ways of handling this
		- statistical methods we design to satisfy average response times
		- if it is rare that the system has timing faults then special cases can be included in the handling code
		- e.g user presses a keyboard
- Cyclic executives (CEs)
	- handles periodic processes
	- Sporadic processes can be converted into equivalent periodic processes or they can be ignored (if they take only a little time to handle)
	- Pre-schedules - a feasible execution schedule is calculated before run time
	- The cyclic executive carries out this schedule
	- it is periodic
	- Highly predictable - non pre-emptible
	- Inflexible, difficult to maintain
	- CE Schedule
		- Major schedule - cyclic allocation of processor blocks which satisfies all deadlines and periods
		- ![[Pasted image 20240819230151.png]]
		- Minor cycle (or frame)  - major schedules are divided into equal size frames. Clock ticks only occur on the frame boundaries
	- Periodic Processes 
		- A = (1, 10, 10) B = (3, 10, 10), C = (2, 20, 20) D = (8, 20, 20)
		- Major cycle time is 20 (smallest possible value we can use in this case). LCM of periods.
		- Frame time - can be 10, GCD of periods
		- ![[Pasted image 20240819230232.png]]
	- Scheduling with Priorities
		- Scheduling decisions are made
			- When a process becomes ready
			- wehn a process completes it execution
			- when there is an interrupt
		- Priorities can cause schedules to not be feasible
			- A(1,2,2) better priority
			- B(2,5,5) worse priority
		- This is feasible (without preemption) but if the priorities are reverssed it is not
		- Still priorities are almost always used
			- fixed - determined before execution
			- dynamic - change during execution
	- Priority Allocation
		- Fixed
			- Rate monotonic (RM) - the shorter the period the higher the priority
			- Least compute time (LCT) - the shorter the execution time the higher the priority (shortest job first)
		- Dynamic
			- Shorted completion time (SCT) - shortest job first with preemption. But this time we have good information on the execution time requirement
			- Earliest deadline first (EDF) - the process with the closest deadline has the highest priority
			- Least slack time (LST) - the process with the least slack time has the ighest priority
				- Slack time is the amount of time to the process deadline minus the amount of time the process still needs to complete



